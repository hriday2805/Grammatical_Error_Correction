# -*- coding: utf-8 -*-
"""Grammatical Error Correction using T5 in HuggingFace ðŸ¤— by Neuralearn.ai--.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yDEGNJ5fTqf8zYiEnr1IzU4UPR9Y3S6y
"""

!pip install transformers datasets evaluate
!pip install sentencepiece

!pip install sacrebleu

import tensorflow as tf### models
import numpy as np### math computations
import io
import os
import re
import matplotlib.pyplot as plt### plotting bar chart
import string
import evaluate
import time
from numpy import random
import tensorflow_datasets as tfds
import tensorflow_probability as tfp
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer
from tensorflow.keras.layers import Dense,Flatten,InputLayer
from tensorflow.keras.optimizers import Adam
from google.colab import drive
from google.colab import files
from datasets import load_dataset
from transformers import create_optimizer,T5TokenizerFast,DataCollatorForSeq2Seq,TFT5ForConditionalGeneration,TFAutoModelForSeq2SeqLM,AutoModelForSeq2SeqLM,TFT5ForConditionalGeneration

BATCH_SIZE=64
MAX_LENGTH=128

"""# Data Preparation"""

#dataset_id="liweili/c4_200m"
dataset_id="leslyarun/c4_200m_gec_train100k_test25k"

dataset = load_dataset(dataset_id)

dataset

dataset['train'][0]

model_id="t5-small"
tokenizer=T5TokenizerFast.from_pretrained(model_id)

def preprocess_function(examples):

  inputs = [example for example in examples['input']]
  targets = [example for example in examples['output']]

  model_inputs = tokenizer(inputs, text_target=targets,max_length=MAX_LENGTH, truncation=True)
  return model_inputs

tokenized_dataset=dataset.map(preprocess_function,batched=True,remove_columns=dataset["train"].column_names)

tokenized_dataset

tokenized_dataset['train'][1000]

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_id)
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,model=model, return_tensors="tf")

train_dataset=tokenized_dataset["train"].to_tf_dataset(
    shuffle=True,
    batch_size=BATCH_SIZE,
    collate_fn=data_collator,
)

val_dataset=tokenized_dataset["test"].to_tf_dataset(
    shuffle=False,
    batch_size=BATCH_SIZE,
    collate_fn=data_collator,
)

for i in val_dataset.take(1):
  print(i)

"""# Modeling"""

model.summary()

"""# Training"""

num_epochs = 5
num_train_steps=len(train_dataset)*num_epochs

optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
)
model.compile(optimizer=optimizer)

history=model.fit(
  train_dataset,
  validation_data=val_dataset,
  epochs=num_epochs
)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model_loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

model.save_weights('/content/drive/MyDrive/nlp/gec/t5-small.h5')

"""# Evaluation"""

metric = evaluate.load("sacrebleu")

all_preds = []
all_labels = []

for batch in val_dataset.take(5):
  print(batch)
  predictions = model.generate(
      input_ids=batch["input_ids"], attention_mask=batch["attention_mask"]
  )
  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
  labels = batch["labels"].numpy()
  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
  all_preds.extend(decoded_preds)
  all_labels.extend(decoded_labels)

result = metric.compute(predictions=all_preds, references=all_labels)
print(result)

wrong_english=[
    "Dady hav'e eateing her foot",
    "DJ Sorryyouwastedyourmoneytobehere",
    "i used to like to swimming",
    "maybe we should organized a meetin with the people from unesco",
    "when are we goinge to start play football",
    "many a time rain fall in my city",
   ]
tokenized=tokenizer(
  wrong_english,
  padding="longest",
  max_length=MAX_LENGTH,
  truncation=True,
  return_tensors='pt'
)
out=pretrained_model.generate(**tokenized, max_length=128)
print(out)

all_preds[1:3]

decoded_preds

decoded_labels



"""# Testing

"""

wrong_english=[
    "Dady hav'e eateing her foot",
    "DJ Sorryyouwastedyourmoneytobehere",
    "i used to like to swimming",
    "maybe we should organized a meetin with the people from unesco",
    "when are we goinge to start play football",
    "many a time rain fall in my city"
    ]
tokenized=tokenizer(
  wrong_english,
  padding="longest",
  max_length=MAX_LENGTH,
  truncation=True,
  return_tensors='tf'
)
out = model.generate(**tokenized, max_length=128)
print(out)

for i in range(len(wrong_english)):
  print(wrong_english[i]+"------------>"+tokenizer.decode(out[i], skip_special_tokens=True))

"""# Testing on Pretrained Model (No FineTuning)"""

pretrained_model=AutoModelForSeq2SeqLM.from_pretrained(
    "juancavallotti/t5-base-gec"
)

wrong_english=[
    "Dady hav'e eateing her foot",
    "DJ Sorryyouwastedyourmoneytobehere",
    "i used to like to swimming",
    "maybe we should organized a meetin with the people from unesco",
    "when are we goinge to start play football",
    "many a time rain fall in my city",
   ]
tokenized=tokenizer(
  wrong_english,
  padding="longest",
  max_length=MAX_LENGTH,
  truncation=True,
  return_tensors='pt'
)
out=pretrained_model.generate(**tokenized, max_length=128)
print(out)

for i in range(len(wrong_english)):
  print(wrong_english[i]+"------------>"+tokenizer.decode(out[i], skip_special_tokens=True))



"""# Deprecated Code

## Data Download
"""

!pip install -q kaggle
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json
!kaggle datasets download -d a0155991rliwei/c4-200m
!unzip "/content/c4-200m.zip" -d "/content/dataset/"

"""## Data Preparation"""

max_source_length = 128
max_target_length = 128

raw_dataset = tf.data.TFRecordDataset('/content/dataset/c4200m/1.0.0/c4200m-train.tfrecord-00001-of-00512')

for raw_record in raw_dataset.take(2):
    print(repr(raw_record))

# Create a description of the features.
feature_description = {

    'input': tf.io.FixedLenFeature([], tf.string, default_value=''),
    'output': tf.io.FixedLenFeature([], tf.string, default_value=''),
}
def _parse_function(example_proto):
  # Parse the input `tf.train.Example` proto using the dictionary above.
  return tf.io.parse_single_example(example_proto, feature_description)

parsed_dataset = raw_dataset.map(_parse_function)
parsed_dataset

for parsed_record in parsed_dataset.take(1):
    #print(repr(parsed_record))
    pass
print(parsed_record['input'].numpy())
print(parsed_record['output'].numpy())

tokenizer = T5Tokenizer.from_pretrained("t5-base")
len(tokenizer.get_vocab())

inp_list=[]
for parsed_record in parsed_dataset:
  inp_list.append([parsed_record['input'].numpy().decode("utf-8"),parsed_record['output'].numpy().decode("utf-8")])

len(inp_list)

header = ['input','attention_mask','output']

with open('c4_200_00001.csv', 'w', encoding='UTF8', newline='') as f:
    writer = csv.writer(f)
    # write the header
    writer.writerow(header)

    encoding = tokenizer(
      list(np.array(inp_list)[:,0]),
      padding="longest",
      max_length=max_source_length,
      truncation=True,
    )

    target_encoding = tokenizer(
      list(np.array(inp_list)[:,1]),
      padding="longest",
      max_length=max_source_length,
      truncation=True,
    )
    final_list=[[encoding.input_ids[i],encoding.attention_mask[i],target_encoding.input_ids[i]] for i in range(len(encoding.input_ids))]
    writer.writerows(final_list)

!cp /content/c4_200_00001.csv /content/drive/MyDrive/gec_dataset

data= pd.read_csv("/content/drive/MyDrive/gec_dataset/c4_200_00000.csv")

print(data['input'][1001])
print(len(list(map(int,data['input'][100][1:-1].split(", ")))))
print(list(map(int,data['input'][100][1:-1].split(", "))))
print(list(map(int,data['attention_mask'][100][1:-1].split(", "))))
print(list(map(int,data['output'][100][1:-1].split(", "))))

len(data)

class DataGenerator(tf.keras.utils.Sequence):
    def __init__(self,data,batch_size,):
        self.batch_size=batch_size
        self.data=data

    def __len__(self):
        return len(self.data)//self.batch_size

    def __getitem__(self,idx):
        x,y=self.data_generation(idx)
        return x,y

    def data_generation(self,idx):

      x_1=[]
      x_2=[]
      y=[]

      for j in range(idx*self.batch_size,(idx+1)*self.batch_size):
          x_1.append(list(map(int,data['input'][j][1:-1].split(", "))))
          x_2.append(list(map(int,data['attention_mask'][j][1:-1].split(", "))))
          y.append(list(map(int,data['output'][j][1:-1].split(", "))))

      input_ids=np.array(x_1)
      attention_mask=np.array(x_2)
      decoder_input_ids=np.array(tf.concat([tf.zeros([self.batch_size,1],dtype=tf.int32),np.array(y)[:,0:-1]],axis=-1))
      labels=np.array(y)

      return [input_ids,attention_mask,decoder_input_ids],labels

BATCH_SIZE=16
train_gen=DataGenerator(data,BATCH_SIZE,)
#val_gen=DataGenerator(val_images,BATCH_SIZE,tokenizer,data_dict,starttoken,INPUT_DIM)

"""## Modeling"""

model = TFT5ForConditionalGeneration.from_pretrained("t5-base")
model.summary()

model.compile(
    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5),
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    #run_eagerly=True,
)
model.fit(train_gen,epochs=1)

model.load_weights('/content/drive/MyDrive/gec_dataset/t5-base-00000-gec.h5')

tokenizer = T5Tokenizer.from_pretrained("t5-base")

#wrong_english=["i thought you come yesterday to come collect the cars?","do you really want to saw me go.",
wrong_english=["Dady hav'e eateing her foot",
               "DJ Sorryyouwastedyourmoneytobehere",
               "i used to like to swimming",
               "maybe we should organized a meetin with the people from unesco",
               "when are we goinge to start play football",
               "many a time rain fall in my city"]
encoding = tokenizer(
      wrong_english,#list(np.array(inp_list)[:,0]),
      padding="longest",
      max_length=max_source_length,
      truncation=True,

    )

outputs=model.generate(np.array(encoding['input_ids']))

for i in range(len(wrong_english)):
  print(wrong_english[i]+"------------>"+tokenizer.decode(outputs[i], skip_special_tokens=True))

















# from transformers import T5Tokenizer, TFT5ForConditionalGeneration

# tokenizer = T5Tokenizer.from_pretrained("t5-small")
# model = TFT5ForConditionalGeneration.from_pretrained("t5-small")

# model.summary()
# # the following 2 hyperparameters are task-specific
# max_source_length = 64
# max_target_length = 64

# # Suppose we have the following 2 training examples:
# input_sequence_1 = "i really loves mangoes."
# output_sequence_1 = "i really love mangoes."

# input_sequence_2 = "whenever you saw me, run?"
# output_sequence_2 = "whenever you see me, run!"

# # encode the inputs
# input_sequences = [input_sequence_1,input_sequence_2]

# encoding = tokenizer(
#     [input_sequence_1,input_sequence_2],

#     padding="longest",
#     max_length=max_source_length,a
#     truncation=True,
#     return_tensors="tf",

#     )

# input_ids, attention_mask = encoding.input_ids, encoding.attention_mask
# print('input_ids',input_ids)
# print(attention_mask)
# # encode the targets
# target_encoding = tokenizer(
#     [output_sequence_1,output_sequence_2],
#     padding="longest",
#     max_length=max_source_length,
#     truncation=True,
#     return_tensors="tf",
#     )
# labels = target_encoding.input_ids
# print('labels',labels)

# from transformers import T5Tokenizer, TFT5ForConditionalGeneration

# tokenizer = T5Tokenizer.from_pretrained("t5-small")
# model = TFT5ForConditionalGeneration.from_pretrained("t5-small")

# model.summary()
# # the following 2 hyperparameters are task-specific
# max_source_length = 64
# max_target_length = 64

# # Suppose we have the following 2 training examples:
# input_sequence_1 = "i really loves mangoes."
# output_sequence_1 = "i really love mangoes."

# input_sequence_2 = "whenever you saw me, run?"
# output_sequence_2 = "whenever you see me, run!"

# # encode the inputs
# input_sequences = [input_sequence_1,input_sequence_2]

# encoding = tokenizer(
#     [input_sequence_1,input_sequence_2],

#     padding="longest",
#     max_length=max_source_length,a
#     truncation=True,
#     return_tensors="tf",

#     )

# input_ids, attention_mask = encoding.input_ids, encoding.attention_mask
# print('input_ids',input_ids)
# print(attention_mask)
# # encode the targets
# target_encoding = tokenizer(
#     [output_sequence_1,output_sequence_2],
#     padding="longest",
#     max_length=max_source_length,
#     truncation=True,
#     return_tensors="tf",
#     )
# labels = target_encoding.input_ids
# print('labels',labels)